{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the raw data\n",
    "data = pd.read_csv('./multipleChoiceResponses.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "#get currency conversion rates\n",
    "conv = pd.read_csv('./conversionRates.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "#get continents\n",
    "cont = pd.read_csv('./ContinentMapping.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16716, 228)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see how large  the survey data is \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5619, 228)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subset to only workers who are working full time\n",
    "ft_data = data.loc[data['EmploymentStatus']=='Employed full-time'  ]\n",
    "\n",
    "#another flag for people who are not working\n",
    "ft_data = ft_data.loc[ft_data['SalaryChange'] !='I am not currently employed']\n",
    "\n",
    "#get rid of anyone who doesn't list a job satisfaction rating as it is the dependent variable of interest\n",
    "final_data = ft_data.loc[ft_data['JobSatisfaction'].notnull() ==True ]\n",
    "\n",
    "#these removals take us down to 5,619 responses\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert other currencies to USD so they can rationally be compared \n",
    "final_data = pd.merge(final_data, conv, how='left', left_on = 'CompensationCurrency', right_on = 'originCountry')\n",
    "final_data['CompensationAmount']= final_data['CompensationAmount'].str.replace( ',', '')\n",
    "final_data['CompensationAmount']= final_data['CompensationAmount'].str.replace( '-', '')\n",
    "final_data['CompensationAmount']= final_data['CompensationAmount'].str.replace( '-1', '')\n",
    "final_data['CompensationAmount']= final_data['CompensationAmount'].str.replace( '-99', '')\n",
    "final_data['CompensationUSD'] =  pd.to_numeric(final_data['CompensationAmount']) * final_data.exchangeRate\n",
    "#final_data['CompensationUSD'] = final_data.loc[[((final_data['CompensationUSD'] <= 500000) | (final_data['CompensationUSD'].empty() == True))]]\n",
    "\n",
    "#discovered two bad actors in the data by looking at observations that had both extreme compensation and\n",
    "#nonsensical results\n",
    "#final_data.sort_values(by='CompensationUSD', ascending = 0)\n",
    "final_data = final_data.drop([2623, 1359])\n",
    "\n",
    "#drop uneeded compensation columns\n",
    "final_data = final_data.drop(['CompensationAmount', 'originCountry', 'exchangeRate', 'CompensationCurrency'], axis=1)\n",
    "#further analysis showed that it is missing for 1/3 of the data and might not be interpreted right so dropping it\n",
    "#final_data['CompensationUSD'].isnull().sum()\n",
    "final_data = final_data.drop('CompensationUSD', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_count_avg = final_data.groupby(by='Country')['CompensationUSD'].agg(['mean'])\n",
    "#df_count_avg.columns = ['Average Salary Within Country']\n",
    "#final_data = pd.merge(final_data, df_count_avg, left_on = 'Country', right_index =True)\n",
    "#final_data['Percent Above/Below Average Salary'] = (final_data['CompensationUSD']/['Average Salary Within Country']-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count the number of challenges someone lists (of any kind)\n",
    "final_data['NumberChallenges'] = final_data.loc[:,'WorkChallengeFrequencyPolitics':'WorkChallengeFrequencyOtherSelect'].count(axis=1)\n",
    "final_data = final_data.drop('WorkChallengesSelect', axis=1)\n",
    "\n",
    "#count the number of platforms someone has access to \n",
    "final_data['NumberPlatforms'] = final_data.loc[:,'LearningPlatformUsefulnessArxiv':'LearningPlatformUsefulnessYouTube'].count(axis=1)\n",
    "final_data = final_data.drop('LearningPlatformSelect', axis=1)\n",
    "\n",
    "#text field that aren't useful for our analysis \n",
    "final_data = final_data.drop('WorkDatasetsChallenge', axis=1)\n",
    "final_data = final_data.drop('WorkDatasets', axis=1)\n",
    "\n",
    "#not something organizations can act on \n",
    "final_data = final_data.drop('PublicDatasetsSelect', axis=1)\n",
    "final_data = final_data.drop('BlogsPodcastsNewslettersSelect', axis=1)\n",
    "final_data = final_data.drop('PastJobTitlesSelect', axis=1)\n",
    "\n",
    "#not easily usable\n",
    "final_data = final_data.drop('WorkMethodsFrequencySelect3', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifies all variables that use the term 'Rarely' for recoding later\n",
    "rarely_list = []\n",
    "for col in final_data.select_dtypes(include={'object'}).columns:\n",
    "    if final_data[col].str.contains('Rarely').any():\n",
    "        rarely_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds additional columsn that need to have recodes done and adds them to the list\n",
    "WorkTools = [col for col in final_data.columns if 'WorkTools' in col]\n",
    "WorkMethods  = [col for col in final_data.columns if 'WorkMethods' in col]\n",
    "WorkChallenge = [col for col in final_data.columns if 'WorkChallenge' in col]\n",
    "\n",
    "biglist = rarely_list+WorkTools+WorkMethods+WorkChallenge\n",
    "biglist = list(set(biglist))\n",
    "biglist.sort()\n",
    "len(biglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recodes everything that is missing to 'Never' since not checking it means roughly the same thing as chekcing 'Never'\n",
    "final_data[biglist] = final_data[biglist].replace(np.nan, 'Never')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converts don't knows into 'missing' as don't know doesn't really grant meaningful information\n",
    "final_data[['WorkProductionFrequency', 'RemoteWork']] = final_data[['WorkProductionFrequency', 'RemoteWork']].replace(\n",
    "    [\"Don't know\"], [\"Missing\"])\n",
    "\n",
    "#[\"Don't know\", 'Never', 'Rarely', 'Sometimes', 'Often', 'Most of the time']\n",
    "final_data = final_data.replace([\"Don't know\"], [\"Missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually created a mapping countries to continents \n",
    "final_data = pd.merge(final_data, cont, how='left', left_on = 'Country', right_on = 'Country')\n",
    "\n",
    "#dropping country data as it is too sparse to be used outside of a few large countries\n",
    "final_data = final_data.drop(['Country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes a dataframe and a specified column that is a comma separated list of values\n",
    "#splits out the lists present in the survey into interpretable dummy variables \n",
    "#returns a dataframe with the new data\n",
    "\n",
    "def CSVListToDummy (df, col):\n",
    "    full_dummy= pd.DataFrame()\n",
    "    for i in range(0, df[col].str.split(',', expand=True).shape[1]):\n",
    "        #Other is common and used across multiple varialbes and is generally useless so dropping\n",
    "        temp = pd.get_dummies(df[col].str.split(',', expand=True)[i])\n",
    "        try:\n",
    "            temp = temp.drop('Other', axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        full_dummy = full_dummy.radd(temp, fill_value  = 0)\n",
    "        temp = None\n",
    "    return full_dummy\n",
    "\n",
    "#create dummy variables for a number of variables \n",
    "WorkToolDummy= CSVListToDummy (final_data, 'WorkToolsSelect') \n",
    "MLSkillsDummy= CSVListToDummy (final_data, 'MLSkillsSelect')  \n",
    "HardwareDummy = CSVListToDummy (final_data, 'WorkHardwareSelect')  \n",
    "WorkAlgoDummy = CSVListToDummy (final_data, 'WorkAlgorithmsSelect')\n",
    "WorkCodeSharingDummy = CSVListToDummy (final_data, 'WorkCodeSharing')\n",
    "EmployerTypeDummy = CSVListToDummy (final_data, 'CurrentEmployerType')\n",
    "MLToolDummy = CSVListToDummy (final_data, 'MLTechniquesSelect')\n",
    "StorageDummy = CSVListToDummy (final_data, 'WorkDataStorage')\n",
    "SharingDummy = CSVListToDummy (final_data, 'WorkDataSharing')\n",
    "SourcingDummy = CSVListToDummy (final_data, 'WorkDataSourcing')\n",
    "WorkDummy = CSVListToDummy (final_data, 'WorkMethodsSelect') \n",
    "\n",
    "#creates 'total count' variables that are sums of all of the dummy variables that are created above\n",
    "WorkToolCount = pd.DataFrame(WorkToolDummy.sum(axis=1), columns = {'Tools Used At Work Count'})\n",
    "MLSkillCount = pd.DataFrame(MLSkillsDummy.sum(axis=1), columns = {'ML Skill Count'})\n",
    "HardwareCount = pd.DataFrame(HardwareDummy.sum(axis=1), columns = {'Hardware Types Used'})\n",
    "WorkAlgoCount = pd.DataFrame(WorkAlgoDummy.sum(axis=1), columns = {'Number of Algorithims'})\n",
    "WorkCodeSharingCount = pd.DataFrame(WorkCodeSharingDummy.sum(axis=1), columns = {'Number of Code Sharing Methods'})\n",
    "MLToolCount = pd.DataFrame(MLToolDummy.sum(axis=1), columns = {'Number of Machine Learning Methods'})\n",
    "StorageCount = pd.DataFrame(StorageDummy.sum(axis=1), columns = {'Number of Storage Options'})\n",
    "SharingCount = pd.DataFrame(SharingDummy.sum(axis=1), columns = {'Number of Sharing Options'})\n",
    "SourcingCount = pd.DataFrame(SourcingDummy.sum(axis=1), columns = {'Number of Sourcing Options'})\n",
    "WorkCount = pd.DataFrame(WorkDummy.sum(axis=1), columns = {'Number of Work Methods'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge all of the new dummy data back into the original dataframe\n",
    "final_data = final_data.join(WorkToolDummy).join(MLSkillsDummy).join(WorkToolCount).join(MLSkillCount).join(HardwareDummy).join(HardwareCount).join(WorkAlgoDummy).join(WorkAlgoCount).join(WorkCodeSharingDummy).join(WorkCodeSharingCount).join(EmployerTypeDummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the original text columns as they're no longer needed \n",
    "final_data = final_data.drop(['WorkToolsSelect','MLSkillsSelect','WorkHardwareSelect', 'WorkAlgorithmsSelect', 'WorkCodeSharing', 'CurrentEmployerType', 'MLTechniquesSelect', 'WorkDataStorage', 'WorkDataSharing', 'WorkDataSourcing', 'WorkMethodsSelect' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop columns that do not have a value for at least 200 values as it could drive overfitting in a population this small\n",
    "final_data = final_data.dropna(axis=1, how='all', thresh=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5617, 275)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the rarely-most of the time scales to a 4 point scale. There are many columns so apply to the entire dataframe \n",
    "#seemed like a good idea but the model actually performs much worse in practice so getting ride of it\n",
    "#final_data= final_data.replace( 'Rarely', '1')\n",
    "#final_data= final_data.replace( 'Sometimes', '2')\n",
    "#final_data= final_data.replace( 'Often', '3')\n",
    "#final_data= final_data.replace( 'Most of the time', '4')\n",
    "#final_data['WorkToolsFrequencyJupyter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleanse Job Satisfaction so that it is a numberic variable that can be a dependent variable in our Random Forest\n",
    "final_data['JobSatisfaction'].replace(to_replace = '1 - Highly Dissatisfied', value = '1', inplace = True)\n",
    "final_data['JobSatisfaction'].replace(to_replace = '10 - Highly Satisfied', value = '10', inplace = True)\n",
    "final_data['JobSatisfaction'].replace(to_replace = 'I prefer not to share', value = None, inplace = True)\n",
    "final_data['JobSatisfaction'] = pd.to_numeric(final_data['JobSatisfaction'])\n",
    "#final_data['JobSatisfaction'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make compensation a meaningful variable\n",
    "final_data['SalaryChange'].replace(to_replace = 'I do not want to share information about my salary/compensation', value = '', inplace = True)\n",
    "\n",
    "#making employer size changes into a 5 point Likurt scale\n",
    "#final_data['EmployerSizeChange'].replace(to_replace = 'Increased significantly', value = '5', inplace = True)\n",
    "#final_data['EmployerSizeChange'].replace(to_replace = 'Increased slightly', value = '4', inplace = True)\n",
    "#final_data['EmployerSizeChange'].replace(to_replace = 'Stayed the same', value = '3', inplace = True)\n",
    "#final_data['EmployerSizeChange'].replace(to_replace = 'Decreased slightly', value = '2', inplace = True)\n",
    "#final_data['EmployerSizeChange'].replace(to_replace = 'Decreased significantly', value = '1', inplace = True)\n",
    "#final_data['EmployerSizeChange'] =  pd.to_numeric(final_data['EmployerSizeChange'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing salary change information into a Likurt scale \n",
    "#final_data['SalaryChange'].replace(to_replace = 'Has increased 20% or more', value = '5', inplace = True)\n",
    "#final_data['SalaryChange'].replace(to_replace = 'Has increased between 6% and 19%', value = '4', inplace = True)\n",
    "#final_data['SalaryChange'].replace(to_replace = 'Has stayed about the same (has not increased or decreased more than 5%)', value = '3', inplace = True)\n",
    "#final_data['SalaryChange'].replace(to_replace = 'Has decreased between 6% and 19%', value = '2', inplace = True)\n",
    "#final_data['SalaryChange'].replace(to_replace = 'Has decreased 20% or more', value = '1', inplace = True)\n",
    "#final_data['SalaryChange'].replace(to_replace = ['I was not employed 3 years ago', 'Other'], value = None, inplace = True)\n",
    "#final_data['SalaryChange'] =  pd.to_numeric(final_data['SalaryChange'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing title fit information into a Likurt scale \n",
    "#final_data['TitleFit'].replace(to_replace = 'Perfectly', value = '3', inplace = True)\n",
    "#final_data['TitleFit'].replace(to_replace = 'Fine', value = '2', inplace = True)\n",
    "#final_data['TitleFit'].replace(to_replace = 'Poorly', value = '1', inplace = True)\n",
    "#final_data['TitleFit'] =  pd.to_numeric(final_data['TitleFit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine non-binary gender categories as they are sparse \n",
    "final_data['GenderSelect'].replace(to_replace = 'Non-binary, genderqueer, or gender non-conforming', value = 'A different identity', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting binary variables into strings so they are processed correctly as categories later on in modeling \n",
    "temp = final_data.max()\n",
    "dftemp = temp.to_frame('max')\n",
    "binaries = dftemp[dftemp['max'] == 1].index\n",
    "#final_data[binaries].apply(str)\n",
    "for i in binaries:\n",
    "    final_data[i] = final_data[i].apply(str)\n",
    "    final_data[i].replace([\"0.0\", '1.0'], [\"No\", \"Yes\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill NAs with the mean value so the rows don't get dropped by potential future classifiers \n",
    "final_data = final_data.fillna(final_data.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill anything missing with a Missing value to clearly designate it \n",
    "#final_data = final_data.fillna('Missing')\n",
    "final_data['Continent']= final_data['Continent'].str.replace( 'Missing', 'Other')\n",
    "\n",
    "#final_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#too related to job satisfaction which is the dependent variable later on so dropping it \n",
    "final_data = final_data.drop('TitleFit', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5617, 274)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Expert', 'Competent', 'Beginner', nan], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['AlgorithmUnderstandingLevel'].unique()\n",
    "final_data['AlgorithmUnderstandingLevel'] = final_data['AlgorithmUnderstandingLevel'].replace(\n",
    "    ['Enough to code it from scratch and it will run blazingly fast and be super efficient', \n",
    "     'Enough to code it again from scratch, albeit it may run slowly',\n",
    "    'Enough to refine and innovate on the algorithm',\n",
    "     'Enough to explain the algorithm to someone non-technical',\n",
    "     'Enough to tune the parameters properly',\n",
    "     'Enough to run the code / standard library'\n",
    "    ], ['Expert', 'Expert', 'Expert', 'Competent', 'Beginner', 'Beginner'])\n",
    "final_data['AlgorithmUnderstandingLevel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write data out for example notebook\n",
    "final_data.to_csv('final_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
